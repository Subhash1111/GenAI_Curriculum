{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1ea4901",
   "metadata": {},
   "source": [
    "# Session 7 â€” Data Analysis & Feature Engineering (NumPy + Pandas)\n",
    "\n",
    "**Goal:** bridge Data Engineering to AI/ML by learning how to **inspect, clean, and transform** data into **model-ready features**.\n",
    "\n",
    "**You will learn**\n",
    "- Core **NumPy** concepts for efficient numeric computation\n",
    "- Practical **Pandas** patterns for cleaning, joining, aggregating\n",
    "- **EDA** (exploratory data analysis) workflow and pitfalls (leakage, imbalance)\n",
    "- Common **feature engineering** techniques across domains (retail, fintech, healthcare, marketing, text, time series)\n",
    "- Packaging features for ML pipelines (hand-off to Session 8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6676cc5d",
   "metadata": {},
   "source": [
    "## ğŸ§­ 1ï¸âƒ£ Why This Matters\n",
    "Data engineers enable ML by delivering **high-quality, well-shaped features**. Even simple models beat complex ones if the **features** are strong.\n",
    "\n",
    "**Key principles:**\n",
    "- Make transformations **reproducible** (deterministic code, versioned)\n",
    "- Avoid **data leakage** (no future info in training)\n",
    "- Capture **business logic** faithfully (domain-aware features)\n",
    "- Keep **data lineage** and **quality checks** from earlier sessions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e05f24e",
   "metadata": {},
   "source": [
    "## ğŸ§± 2ï¸âƒ£ NumPy Foundations (Arrays & Vectorization)\n",
    "NumPy provides fast **n-dimensional arrays** and vectorized operations.\n",
    "\n",
    "**Concepts:**\n",
    "- Array creation, dtypes, shapes\n",
    "- Broadcasting: operations across different shapes\n",
    "- Aggregations: `mean`, `sum`, `std`, `percentile`\n",
    "- Boolean masks and indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eae1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.arange(4)  # [0,1,2,3]\n",
    "\n",
    "print('a:', a, 'shape:', a.shape)\n",
    "print('b:', b)\n",
    "print('a + b:', a + b)\n",
    "print('broadcast add (a + 10):', a + 10)\n",
    "print('mean(a):', a.mean(), 'std(a):', a.std())\n",
    "\n",
    "# boolean masking\n",
    "mask = a % 2 == 0\n",
    "print('even values in a:', a[mask])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95c1cb3",
   "metadata": {},
   "source": [
    "## ğŸ¼ 3ï¸âƒ£ Pandas for Tabular Data\n",
    "**Pandas** wraps NumPy with labels and table-like operations.\n",
    "\n",
    "**Patterns youâ€™ll use daily:**\n",
    "- Import/export: CSV, JSON, (Parquet optional)\n",
    "- Selecting, filtering, sorting, renaming\n",
    "- Handling missing values (drop/impute)\n",
    "- GroupBy aggregations\n",
    "- Joins/merges and reshaping (pivot/melt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d527b76",
   "metadata": {},
   "source": [
    "### âœï¸ Mini Retail Dataset (inline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3180d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data = [\n",
    "    {'order_id':1,'customer_id':'C-101','region':'North','sku':'LAPTOP-15','qty':2,'price':1200.0,'order_dt':'2025-10-01'},\n",
    "    {'order_id':2,'customer_id':'C-102','region':'South','sku':'MOUSE-01','qty':1,'price':25.0,'order_dt':'2025-10-01'},\n",
    "    {'order_id':3,'customer_id':'C-103','region':'North','sku':'LAPTOP-15','qty':1,'price':1250.0,'order_dt':'2025-10-02'},\n",
    "    {'order_id':4,'customer_id':'C-104','region':'West','sku':'KB-01','qty':1,'price':45.0,'order_dt':'2025-10-02'},\n",
    "    {'order_id':5,'customer_id':'C-101','region':'North','sku':'LAPTOP-15','qty':1,'price':1180.0,'order_dt':'2025-10-03'},\n",
    "]\n",
    "df = pd.DataFrame(data)\n",
    "df['order_dt'] = pd.to_datetime(df['order_dt'])\n",
    "df['amount'] = df['qty'] * df['price']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef9fe58",
   "metadata": {},
   "source": [
    "### ğŸ” Typical Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f5702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter + select\n",
    "north_laptop = df[(df['region']=='North') & (df['sku'].str.contains('LAPTOP'))][['order_id','customer_id','amount']]\n",
    "print(north_laptop)\n",
    "\n",
    "# groupby\n",
    "sales_by_region = df.groupby('region', as_index=False)['amount'].sum().sort_values('amount', ascending=False)\n",
    "print('\\nSales by region:\\n', sales_by_region)\n",
    "\n",
    "# pivot (sku x day)\n",
    "pivot = df.pivot_table(index='order_dt', columns='sku', values='amount', aggfunc='sum', fill_value=0)\n",
    "print('\\nPivot (daily amount by SKU):\\n', pivot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089ee80b",
   "metadata": {},
   "source": [
    "## ğŸ§¼ 4ï¸âƒ£ Data Cleaning Playbook\n",
    "- **Missing values**: drop vs. impute (mean/median/mode/domain rules)\n",
    "- **Outliers**: winsorize, cap, or remove if data errors\n",
    "- **Duplicates**: use `drop_duplicates` on business keys\n",
    "- **Types**: cast to numeric/datetime; normalize text case/trim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f455f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate some dirtiness\n",
    "dirt = df.copy()\n",
    "dirt.loc[1,'price'] = None\n",
    "dirt.loc[2,'region'] = None\n",
    "dirt = pd.concat([dirt, dirt.iloc[[0]]], ignore_index=True)  # duplicate row\n",
    "\n",
    "print('Before cleaning:\\n', dirt)\n",
    "\n",
    "# dedupe\n",
    "dirt = dirt.drop_duplicates()\n",
    "# impute price with median per sku\n",
    "dirt['price'] = dirt.groupby('sku')['price'].transform(lambda s: s.fillna(s.median()))\n",
    "# fill region with 'Unknown'\n",
    "dirt['region'] = dirt['region'].fillna('Unknown')\n",
    "# recompute amount\n",
    "dirt['amount'] = dirt['qty'] * dirt['price']\n",
    "\n",
    "print('\\nAfter cleaning:\\n', dirt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee46a07",
   "metadata": {},
   "source": [
    "## ğŸ§ª 5ï¸âƒ£ Feature Engineering â€” Concepts & Patterns\n",
    "**Numerical features**: scaling, binning, log transforms\n",
    "\n",
    "**Categorical features**: one-hot, target encoding (careful: leakage risk)\n",
    "\n",
    "**Datetime features**: year, month, day-of-week, hour; rolling windows; time since last event\n",
    "\n",
    "**Text features**: length, word counts, n-grams, TFâ€“IDF (details in Session 8)\n",
    "\n",
    "**Aggregation features**: per-customer totals, moving averages, ratios\n",
    "\n",
    "**Domain features**: promos, seasonality flags, churn windows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bdb6ca",
   "metadata": {},
   "source": [
    "### ğŸ§± Numerical + Categorical + Datetime Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2517d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical: simple scaling fallback if sklearn unavailable\n",
    "import numpy as np\n",
    "\n",
    "fe = df.copy()\n",
    "fe['price_z'] = (fe['price'] - fe['price'].mean()) / (fe['price'].std(ddof=0))\n",
    "\n",
    "# categorical: one-hot\n",
    "ohe = pd.get_dummies(fe['region'], prefix='region')\n",
    "fe = pd.concat([fe, ohe], axis=1)\n",
    "\n",
    "# datetime: calendar parts\n",
    "fe['dow'] = fe['order_dt'].dt.dayofweek\n",
    "fe['month'] = fe['order_dt'].dt.month\n",
    "\n",
    "# lag/rolling example (by SKU) â€“ requires sorted index\n",
    "fe = fe.sort_values(['sku','order_dt'])\n",
    "fe['amt_rolling_3'] = fe.groupby('sku')['amount'].transform(lambda s: s.rolling(3, min_periods=1).mean())\n",
    "\n",
    "fe[['order_id','sku','amount','price_z','region_North','region_South','region_West','dow','month','amt_rolling_3']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf798add",
   "metadata": {},
   "source": [
    "## âš ï¸ 6ï¸âƒ£ Common Pitfalls (and How to Avoid)\n",
    "- **Data leakage:** no using future info when creating features (respect time order)\n",
    "- **Target leakage:** avoid features derived directly from labels\n",
    "- **Imbalanced data:** stratified splits; balanced class weights\n",
    "- **Spurious correlations:** validate with out-of-time tests\n",
    "- **Overfitting via joins:** ensure one-to-one or aggregate before joining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34de45c1",
   "metadata": {},
   "source": [
    "## ğŸŒ 7ï¸âƒ£ Cross-Domain Feature Examples (Diverse)\n",
    "**Retail** â€” basket size, monthly spend, days since last purchase\n",
    "\n",
    "**FinTech** â€” transaction velocity, merchant diversity, failed-payment rate\n",
    "\n",
    "**Healthcare IoT** â€” rolling mean of vitals, anomaly flags, adherence ratio\n",
    "\n",
    "**Marketing** â€” email open rate, channel diversity index, RFM (recency-frequency-monetary)\n",
    "\n",
    "**Text/NLP** â€” review length, sentiment score, TFâ€“IDF vectors (Session 8)\n",
    "\n",
    "**Time Series** â€” lag features, rolling windows, seasonality dummies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4230757b",
   "metadata": {},
   "source": [
    "## ğŸ–¼ï¸ 8ï¸âƒ£ Visual â€” From Raw Data to Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 3.6))\n",
    "ax.axis('off')\n",
    "\n",
    "labels = [\n",
    "    ('Raw Data', 'tables, logs, files'),\n",
    "    ('Clean', 'types, missing, dedupe'),\n",
    "    ('Enrich', 'joins, business rules'),\n",
    "    ('Engineer', 'ohe, scale, lags'),\n",
    "    ('Features', 'model-ready set')\n",
    "]\n",
    "W, H, GAP, PAD, Y0 = 0.18, 0.22, 0.06, 0.01, 0.39\n",
    "total_w = len(labels)*W + (len(labels)-1)*GAP\n",
    "x0 = (1-total_w)/2\n",
    "xs = [x0 + i*(W+GAP) for i in range(len(labels))]\n",
    "\n",
    "def box(x, t, s):\n",
    "    r = FancyBboxPatch((x, Y0), W, H, boxstyle='round,pad=0.02,rounding_size=10')\n",
    "    ax.add_patch(r)\n",
    "    ax.text(x+W/2, Y0+H*0.62, t, ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    ax.text(x+W/2, Y0+H*0.36, s, ha='center', va='center', fontsize=9)\n",
    "\n",
    "for (t,s), x in zip(labels, xs):\n",
    "    box(x, t, s)\n",
    "\n",
    "y = Y0 + H/2\n",
    "for i in range(len(xs)-1):\n",
    "    ax.annotate('', xy=(xs[i+1]-PAD, y), xytext=(xs[i]+W+PAD, y),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2))\n",
    "\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387825ae",
   "metadata": {},
   "source": [
    "## ğŸ”— 9ï¸âƒ£ Handoff to Session 8 (ML Foundations)\n",
    "- Split **train/test** preserving time order or using stratification\n",
    "- Evaluate with proper metrics (classification vs regression)\n",
    "- Save features and **model artifacts** reproducibly\n",
    "- Integrate with orchestration (Airflow/ADF) and governance (Purview/Glue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e5712",
   "metadata": {},
   "source": [
    "## ğŸ’¡ ğŸ”Ÿ Practice / Assignment\n",
    "1) Create at least **8 features** from the retail dataset (mix of numerical/categorical/datetime/aggregation).\n",
    "2) Simulate **leakage** (incorrect) and then fix it (correct time-aware features).\n",
    "3) Write a short **EDA checklist** (nulls, skews, outliers) for any dataset you choose.\n",
    "4) Save a **features CSV** ready for ML training in Session 8.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
