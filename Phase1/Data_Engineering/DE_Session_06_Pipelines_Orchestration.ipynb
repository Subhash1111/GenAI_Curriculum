{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde81be5",
   "metadata": {},
   "source": [
    "# Session 6 — Pipelines & Orchestration\n",
    "\n",
    "Full vs Incremental loads, job scheduling, monitoring (Airflow concepts), parameterization, and Medallion architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d03b9",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ef3bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, sqlite3, pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "print(sys.version)\n",
    "import seaborn as sns; sns.set_theme()\n",
    "DB_PATH = Path('course.db')\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "conn.execute('PRAGMA foreign_keys=ON;')\n",
    "print('SQLite ready at', DB_PATH.resolve())\n",
    "def run_sql(q, params=None):\n",
    "    params = params or {}\n",
    "    df = pd.read_sql_query(q, conn, params=params)\n",
    "    display(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899893e1",
   "metadata": {},
   "source": [
    "## Full Load vs Incremental Loads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1af5b",
   "metadata": {},
   "source": [
    "**Full Load** copies the entire source into the destination each run.  \n",
    "**Incremental Load** copies only **new or changed** records since the last run.\n",
    "\n",
    "![Full vs Incremental](/mnt/data/images/full_vs_incremental.png)\n",
    "\n",
    "**Why it matters:** Incrementals reduce compute, bandwidth, and downtime; full loads are simpler and good for initial backfills or small tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d975e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: emulate a source table with change tracking (updated_at)\n",
    "conn.executescript('''\n",
    "DROP TABLE IF EXISTS src_customers;\n",
    "DROP TABLE IF EXISTS dw_customers;\n",
    "\n",
    "CREATE TABLE src_customers(\n",
    "  id INTEGER PRIMARY KEY, name TEXT, city TEXT, updated_at TEXT\n",
    ");\n",
    "INSERT INTO src_customers VALUES\n",
    " (1,'Aria','Austin','2024-10-01'),\n",
    " (2,'Ben','Berlin','2024-10-01'),\n",
    " (3,'Chloe','Chicago','2024-10-01');\n",
    "\n",
    "CREATE TABLE dw_customers(\n",
    "  id INTEGER PRIMARY KEY, name TEXT, city TEXT, updated_at TEXT\n",
    ");\n",
    "'''); conn.commit()\n",
    "\n",
    "print(\"FULL LOAD #1 → copy all rows\")\n",
    "conn.executescript(\"INSERT INTO dw_customers SELECT * FROM src_customers;\"); conn.commit()\n",
    "print(\"DW after full load:\"); \n",
    "_ = pd.read_sql_query(\"SELECT * FROM dw_customers\", conn); display(_)\n",
    "\n",
    "print(\"Simulate changes at source (1 update, 1 insert)\")\n",
    "conn.executescript('''\n",
    "UPDATE src_customers SET city='Aurora', updated_at='2024-10-05' WHERE id=3;\n",
    "INSERT INTO src_customers VALUES (4,'Dai','Denver','2024-10-05');\n",
    "'''); conn.commit()\n",
    "\n",
    "print(\"INCREMENTAL LOAD using updated_at watermark\")\n",
    "last_watermark = pd.read_sql_query(\"SELECT MAX(updated_at) AS wm FROM dw_customers;\", conn).iloc[0]['wm']\n",
    "print(\"Last watermark:\", last_watermark)\n",
    "changed = pd.read_sql_query(\"SELECT * FROM src_customers WHERE updated_at > ?\", conn, params=[last_watermark])\n",
    "display(changed)\n",
    "\n",
    "# Upsert (SQLite: replace strategy via INSERT OR REPLACE)\n",
    "conn.executemany(\"INSERT OR REPLACE INTO dw_customers(id,name,city,updated_at) VALUES (?,?,?,?)\", changed.values.tolist())\n",
    "conn.commit()\n",
    "\n",
    "print(\"DW after incremental:\")\n",
    "display(pd.read_sql_query(\"SELECT * FROM dw_customers ORDER BY id\", conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfa71e1",
   "metadata": {},
   "source": [
    "## Job Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dfd646",
   "metadata": {},
   "source": [
    "Schedulers coordinate when pipelines run.\n",
    "![Scheduling](/mnt/data/images/job_scheduling.png)\n",
    "\n",
    "**Common options:** cron, Airflow, Prefect, Dagster, dbt Cloud.  \n",
    "Below are **cron** and an **Airflow-style DAG** (illustrative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc55f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: a cron entry (documentation-only string)\n",
    "cron_example = \"\"\"\n",
    "# Run incremental load at 2:05 AM daily\n",
    "5 2 * * * /usr/bin/python3 /opt/pipelines/incremental_load.py >> /var/log/pipeline.log 2>&1\n",
    "\"\"\"\n",
    "print(cron_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b800cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airflow DAG pseudo-example (doesn't execute here, for reference)\n",
    "airflow_dag = '''from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime\n",
    "\n",
    "def full_load(**context):\n",
    "    # call your SQL scripts / Python to do initial full load\n",
    "    pass\n",
    "\n",
    "def incremental_load(**context):\n",
    "    # pull last watermark, extract changes, upsert\n",
    "    pass\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"customers_etl\",\n",
    "    start_date=datetime(2024,10,1),\n",
    "    schedule_interval=\"@daily\",\n",
    "    catchup=False,\n",
    "    default_args={\"retries\": 1},\n",
    ") as dag:\n",
    "    t1 = PythonOperator(task_id=\"full_load\", python_callable=full_load)\n",
    "    t2 = PythonOperator(task_id=\"incremental_load\", python_callable=incremental_load)\n",
    "    t1 >> t2\n",
    "'''\n",
    "print(airflow_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4371d2",
   "metadata": {},
   "source": [
    "## Monitoring & Observability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e94615",
   "metadata": {},
   "source": [
    "What to track: **task success/failure, duration, data volumes, row counts, anomalies, alerts**.  \n",
    "![Monitoring](/mnt/data/images/monitoring.png)\n",
    "\n",
    "Below are simple **row-count checks** and **alert stubs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8559d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row-count check before/after load\n",
    "before = pd.read_sql_query(\"SELECT COUNT(*) AS n FROM dw_customers\", conn).iloc[0]['n']\n",
    "# simulate another change\n",
    "conn.execute(\"INSERT INTO src_customers VALUES (5,'Eva','El Paso','2024-10-06')\"); conn.commit()\n",
    "# incremental again\n",
    "last_wm = pd.read_sql_query(\"SELECT MAX(updated_at) AS wm FROM dw_customers\", conn).iloc[0]['wm']\n",
    "delta = pd.read_sql_query(\"SELECT * FROM src_customers WHERE updated_at > ?\", conn, params=[last_wm])\n",
    "conn.executemany(\"INSERT OR REPLACE INTO dw_customers(id,name,city,updated_at) VALUES (?,?,?,?)\", delta.values.tolist())\n",
    "conn.commit()\n",
    "after = pd.read_sql_query(\"SELECT COUNT(*) AS n FROM dw_customers\", conn).iloc[0]['n']\n",
    "\n",
    "if after < before:\n",
    "    print(\"ALERT: Row count dropped unexpectedly!\")\n",
    "else:\n",
    "    print(\"OK: Row count non-decreasing.\", before, \"->\", after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2612b42b",
   "metadata": {},
   "source": [
    "## Parameterization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a045350b",
   "metadata": {},
   "source": [
    "Pass **environment-specific parameters** (e.g., schema names, file paths, watermarks) to avoid hard-coding.  \n",
    "![Parameterization](/mnt/data/images/parameterization.png)\n",
    "\n",
    "We'll use a simple dictionary; in production use YAML/JSON or environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c086fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "CONFIG = {\n",
    "    \"watermark_table\": \"dw_customers\",\n",
    "    \"batch_size\": 5000,\n",
    "    \"email_alerts\": False,\n",
    "    \"target_schema\": \"main\"\n",
    "}\n",
    "print(json.dumps(CONFIG, indent=2))\n",
    "\n",
    "def get_watermark(conn, table, col=\"updated_at\"):\n",
    "    q = f\"SELECT MAX({col}) AS wm FROM {table}\"\n",
    "    return pd.read_sql_query(q, conn).iloc[0]['wm']\n",
    "\n",
    "print(\"Current watermark:\", get_watermark(conn, CONFIG[\"watermark_table\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2487b57c",
   "metadata": {},
   "source": [
    "## Medallion Architecture (Bronze → Silver → Gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1282b7",
   "metadata": {},
   "source": [
    "**Bronze:** raw ingested data (minimal changes)  \n",
    "**Silver:** cleaned & conformed data (types fixed, duplicates removed)  \n",
    "**Gold:** business-ready aggregates and marts\n",
    "\n",
    "![Medallion](/mnt/data/images/medallion_architecture.png)\n",
    "\n",
    "Below we stage customer data through bronze→silver→gold using SQLite tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fa9ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.executescript('''\n",
    "DROP TABLE IF EXISTS bronze_customers;\n",
    "DROP TABLE IF EXISTS silver_customers;\n",
    "DROP TABLE IF EXISTS gold_customer_stats;\n",
    "\n",
    "CREATE TABLE bronze_customers AS SELECT * FROM src_customers; -- raw\n",
    "CREATE TABLE silver_customers AS\n",
    "  SELECT DISTINCT id, name, TRIM(city) AS city, updated_at FROM bronze_customers\n",
    "  WHERE name IS NOT NULL; -- simple cleanup\n",
    "CREATE TABLE gold_customer_stats AS\n",
    "  SELECT city, COUNT(*) AS n_customers\n",
    "  FROM silver_customers GROUP BY city;\n",
    "'''); conn.commit()\n",
    "\n",
    "print(\"Bronze sample:\"); display(pd.read_sql_query(\"SELECT * FROM bronze_customers LIMIT 5\", conn))\n",
    "print(\"Silver sample:\"); display(pd.read_sql_query(\"SELECT * FROM silver_customers LIMIT 5\", conn))\n",
    "print(\"Gold sample:\"); display(pd.read_sql_query(\"SELECT * FROM gold_customer_stats\", conn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
