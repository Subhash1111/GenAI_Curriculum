{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n# \ud83c\udf33 Machine Learning Models Hierarchy (with Descriptions)\n\n## 1. By Learning Paradigm\n\n### \ud83d\udd39 1.1 Supervised Learning  \n*(Labeled data \u2192 predict outputs)*  \n\n**Regression (predict continuous values):**\n- **Linear Regression** \u2192 Fits a straight line to predict numeric outcomes.  \n- **Polynomial Regression** \u2192 Extends linear regression by modeling curved relationships.  \n- **Ridge / Lasso / ElasticNet** \u2192 Regularized linear models that prevent overfitting.  \n- **Support Vector Regression (SVR)** \u2192 Uses support vectors to fit regression with margins.  \n- **Decision Tree Regression** \u2192 Splits data into regions using if-else rules.  \n- **Random Forest Regression** \u2192 Combines many decision trees for more stable predictions.  \n- **Gradient Boosting (XGBoost, LightGBM, CatBoost)** \u2192 Builds trees sequentially, improving each step.  \n- **Neural Networks (for regression tasks)** \u2192 Flexible models that can capture complex nonlinear patterns.  \n\n**Classification (predict categories):**\n- **Logistic Regression** \u2192 Estimates probabilities for binary/multiclass outcomes.  \n- **k-Nearest Neighbors (kNN)** \u2192 Classifies based on the majority of nearby points.  \n- **Support Vector Machines (SVM)** \u2192 Finds the best separating boundary (hyperplane).  \n- **Naive Bayes** \u2192 Uses probability with the \u201cindependence\u201d assumption.  \n- **Decision Trees** \u2192 Splits features into branches to classify samples.  \n- **Random Forest** \u2192 A collection of trees voting on the output.  \n- **Gradient Boosted Trees (XGBoost, LightGBM, CatBoost)** \u2192 Sequential trees correcting previous mistakes.  \n- **Neural Networks (MLP, CNN, RNN)** \u2192 Powerful learners for images, text, and sequences.  \n\n\n### \ud83d\udd39 1.2 Unsupervised Learning  \n*(Unlabeled data \u2192 find structure/patterns)*  \n\n**Clustering:**  \n- **k-Means** \u2192 Groups data into k clusters by minimizing distance to centroids.  \n- **Hierarchical Clustering** \u2192 Builds clusters step by step, forming a tree of groups.  \n- **DBSCAN** \u2192 Finds clusters of dense points, marking noise separately.  \n- **Gaussian Mixture Models (GMM)** \u2192 Models data as a mix of probability distributions.  \n\n**Dimensionality Reduction:**  \n- **Principal Component Analysis (PCA)** \u2192 Projects data to fewer dimensions while keeping variance.  \n- **t-SNE** \u2192 Visualizes high-dimensional data in 2D or 3D while preserving neighborhoods.  \n- **UMAP** \u2192 Faster dimensionality reduction, often better than t-SNE for large data.  \n- **Autoencoders** \u2192 Neural networks that compress and reconstruct data.  \n\n**Association Rule Learning:**  \n- **Apriori** \u2192 Finds frequent patterns (e.g., items bought together).  \n- **FP-Growth** \u2192 Faster algorithm for association rule mining.  \n\n\n### \ud83d\udd39 1.3 Semi-Supervised Learning  \n*(Mix of labeled + unlabeled data)*  \n- **Self-training methods** \u2192 Train on labeled data, then iteratively label unlabeled data.  \n- **Semi-supervised SVM** \u2192 Extends SVM to use both labeled and unlabeled points.  \n- **Graph-based methods** \u2192 Spread labels across connected data in a graph structure.  \n\n\n### \ud83d\udd39 1.4 Reinforcement Learning  \n*(Agents learn by interacting with environment)*  \n\n- **Value-based methods:**  \n  - **Q-Learning** \u2192 Learns the expected reward of actions in states.  \n  - **Deep Q-Network (DQN)** \u2192 Uses neural networks to approximate Q-values.  \n\n- **Policy-based methods:**  \n  - **Policy Gradient** \u2192 Directly learns the policy of actions.  \n  - **REINFORCE** \u2192 A simple policy gradient algorithm.  \n\n- **Actor-Critic methods:**  \n  - **A3C** \u2192 Multiple agents train in parallel to stabilize learning.  \n  - **PPO (Proximal Policy Optimization)** \u2192 A robust, widely used RL algorithm.  \n\n**Applications:** Robotics, Games (AlphaGo), Autonomous Driving.  \n\n\n---\n\n## 2. By Model Families\n\n- **Linear Models** \u2192 Models based on linear equations (fast and interpretable).  \n- **Tree-Based Models** \u2192 Nonlinear, interpretable splits on features.  \n- **Instance-Based Models** \u2192 Use similarities to training examples (like kNN).  \n- **Probabilistic Models** \u2192 Estimate probabilities from data distributions.  \n- **Neural Networks** \u2192 Layers of neurons capturing complex, nonlinear patterns.  \n\n\n---\n\n## 3. Advanced / Specialized Models\n\n- **Ensemble Methods** \u2192 Combine multiple models to improve results.  \n  - **Bagging** \u2192 Train many models in parallel and average their outputs.  \n  - **Boosting** \u2192 Train sequentially, each focusing on previous errors.  \n  - **Stacking** \u2192 Combine different model types through another model.  \n\n- **Generative Models** \u2192 Create new data similar to training data.  \n  - **GANs** \u2192 Two networks (generator + discriminator) competing.  \n  - **VAEs** \u2192 Compress and reconstruct data with probabilistic encodings.  \n  - **Diffusion Models** \u2192 Gradually turn noise into realistic data (e.g., Stable Diffusion).  \n\n- **Graph ML** \u2192 Models relationships in graph data (social networks, molecules).  \n\n- **Deep Reinforcement Learning** \u2192 Combines RL with deep neural networks for complex tasks.  \n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}